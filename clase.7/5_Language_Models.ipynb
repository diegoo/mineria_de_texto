{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fOJZbuBfZ3ix",
    "outputId": "d7b3b1c3-2871-476b-b12e-f482dce5fe5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ddellera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import defaultdict\n",
    "from nltk import bigrams, trigrams\n",
    "import random\n",
    "\n",
    "def format_and_clean(text):\n",
    "    text = text.decode('utf-8').lower()\n",
    "    # clean review\n",
    "    text = re.sub('<.*?>',' ',text)\n",
    "    text = re.sub('[,!¡¿?\"]', '', text)\n",
    "\n",
    "    text = sent_tokenize(text)\n",
    "    text = [word_tokenize(s) for s in text]\n",
    "    #text = [x for s in text for x in s if x.isalpha()]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hk8Bp-bAlkOI"
   },
   "outputs": [],
   "source": [
    "# Trabajando con el corpus entero de IMDB\n",
    "\n",
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
    "\n",
    "(train_data, validation_data), test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=(train_validation_split, tfds.Split.TEST),\n",
    "    as_supervised=True)\n",
    "\n",
    "df_train = pd.DataFrame(list(tfds.as_numpy(train_data)),columns=['texto','clase'])\n",
    "df_dev = pd.DataFrame(list(tfds.as_numpy(validation_data)),columns=['texto','clase'])\n",
    "df_test = pd.DataFrame(list(tfds.as_numpy(test_data)),columns=['texto','clase'])\n",
    "\n",
    "X_train_text = list(df_train.texto.apply(format_and_clean))\n",
    "corpusIMDB = sum(X_train_text,[])\n",
    "\n",
    "X_dev_text = list(df_dev.texto.apply(format_and_clean))\n",
    "corpusIMDB_dev = sum(X_dev_text,[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "r785ZApoZWpQ",
    "outputId": "cad0c464-ba89-47b3-ef08-66168115bb8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ddellera/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package reuters to /home/ddellera/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/ddellera/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
     ]
    }
   ],
   "source": [
    "# Trabajando con corpus mas chicos\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('reuters')\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "corpusNLTK = brown.sents() + reuters.sents() + inaugural.sents()\n",
    "corpusBrown = brown.sents() \n",
    "corpusReuters = reuters.sents() \n",
    "corpusInagural = inaugural.sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahxxBHKPmoXo"
   },
   "outputs": [],
   "source": [
    "def train3gram(corpus):\n",
    "      # Create a placeholder for model\n",
    "      model3 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "      # Count frequency of co-occurance \n",
    "\n",
    "      for s in corpus:\n",
    "          for w1, w2, w3 in trigrams(s, pad_right=True, pad_left=True):\n",
    "              model3[(w1, w2)][w3] += 1\n",
    "\n",
    "      for w1_w2 in model3:\n",
    "          total_count = float(sum(model3[w1_w2].values()))\n",
    "          for w3 in model3[w1_w2]:\n",
    "              model3[w1_w2][w3] /= total_count\n",
    "\n",
    "      return model3\n",
    "\n",
    "def generateSentence(model, start):\n",
    "      # starting words\n",
    "      text = start\n",
    "      sentence_finished = False\n",
    "      \n",
    "      while not sentence_finished:\n",
    "        # select a random probability threshold  \n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "\n",
    "        h = tuple(text[-2:])\n",
    "\n",
    "        for w in model[h].keys():\n",
    "            accumulator += model[h][w]\n",
    "            # select words that are above the probability threshold\n",
    "            if accumulator >= r:\n",
    "                text.append(w)\n",
    "                break\n",
    "\n",
    "        if text[-2:] == [None, None] or len(text) == 15:\n",
    "            sentence_finished = True\n",
    "      \n",
    "      print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "15slxdeaVZ5C",
    "outputId": "07fb7957-63d0-4c23-aba0-7a8c53746b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man and petty thief from the uk the original one but the distributors only\n",
      "the man , well , what is possibly no more water '' .\n",
      "the man had `` more monei presente much might be culturally or geographically determined .\n",
      "the man who likes to watch closely various developments resulting from continued weakness in the\n",
      "the man you have called by the late war , but powerful , with full\n"
     ]
    }
   ],
   "source": [
    "modelIMDB = train3gram(corpusIMDB)\n",
    "modelNLTK = train3gram(corpusNLTK)\n",
    "modelBrown = train3gram(corpusBrown)\n",
    "modelReuters = train3gram(corpusReuters)\n",
    "modelInagural = train3gram(corpusInagural)\n",
    "\n",
    "generateSentence(modelIMDB, ['the','man'])\n",
    "generateSentence(modelNLTK, ['the','man'])\n",
    "generateSentence(modelBrown, ['the','man'])\n",
    "generateSentence(modelReuters, ['the','man'])\n",
    "generateSentence(modelInagural, ['the','man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rN6oZC1gGf-k",
    "outputId": "30dc1b69-6999-4c8a-a0f1-fb3adf4c16fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10143.084179100537\n",
      "335070.5936160355\n"
     ]
    }
   ],
   "source": [
    "# Calculemos Perplexity\n",
    "import numpy as np\n",
    "\n",
    "def perplexity(corpus,model):\n",
    "    probs = []\n",
    "    for s in corpus:\n",
    "        for w1, w2, w3 in trigrams(s, pad_right=True, pad_left=True):\n",
    "            p = model[(w1, w2)][w3]\n",
    "            p_smoothed = p + 0.0000001\n",
    "            logp = np.log2(p_smoothed)  \n",
    "            probs.append(logp)\n",
    "\n",
    "    pp = np.exp2(-np.mean(probs))\n",
    "    return pp\n",
    "\n",
    "print(perplexity(corpusIMDB_dev, modelIMDB))\n",
    "print(perplexity(corpusIMDB_dev, modelNLTK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "5-Language Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
