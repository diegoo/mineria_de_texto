{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2UljDWum97h"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.matutils import cossim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from sklearn import manifold\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63h_9SPfm97m"
   },
   "source": [
    "# Word2vec tutorial\n",
    "## Corpus Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbyiNjiL_loe"
   },
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
    "\n",
    "(train_data, validation_data), test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=(train_validation_split, tfds.Split.TEST),\n",
    "    as_supervised=True)\n",
    "\n",
    "df_train = pd.DataFrame(list(tfds.as_numpy(train_data)),columns=['texto','clase'])\n",
    "df_dev = pd.DataFrame(list(tfds.as_numpy(validation_data)),columns=['texto','clase'])\n",
    "df_test = pd.DataFrame(list(tfds.as_numpy(test_data)),columns=['texto','clase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVhphY2_CuMW"
   },
   "outputs": [],
   "source": [
    "df_train.texto[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNarKiJ__7Ib"
   },
   "outputs": [],
   "source": [
    "trainset = []\n",
    "for rev in df_train.texto:\n",
    "  rev = rev.decode('utf-8').lower()\n",
    "  # clean review\n",
    "  rev = re.sub('<.*?>',' ',rev)\n",
    "  #split by sentence\n",
    "  sentences = sent_tokenize(rev)\n",
    "  for sent in sentences:\n",
    "    # word tokenize and append the sentence as a list of words\n",
    "    trainset.append([word for word in word_tokenize(sent) if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "my4dbzYDB3kZ"
   },
   "outputs": [],
   "source": [
    "trainset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwATy5rwm971"
   },
   "outputs": [],
   "source": [
    "print(\"el corpus tiene\",len(trainset), \"oraciones y\",sum([len(x) for x in trainset]),\"palabras\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StXZH1zWPzuq"
   },
   "outputs": [],
   "source": [
    "collocations = Phrases(sentences=trainset, min_count=10,threshold=0.5,scoring='npmi') # threshold: minimo score aceptado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoeeJVZtQe37"
   },
   "outputs": [],
   "source": [
    "to_collocations = Phraser(collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMcicaVvQwTF"
   },
   "outputs": [],
   "source": [
    "df_collocations =pd.DataFrame([x for x in collocations.export_phrases(trainset)],columns=[\"bigram\",\"score\"])\n",
    "df_collocations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eK86uCR1RgRD"
   },
   "outputs": [],
   "source": [
    "df_collocations.drop_duplicates().sort_values(by=\"score\",ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuiuZtUgRpOv"
   },
   "outputs": [],
   "source": [
    "trainset_ngrams = to_collocations[trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-bUh4M-Rz30"
   },
   "outputs": [],
   "source": [
    "trainset_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvYifl8Rm977"
   },
   "outputs": [],
   "source": [
    "# \"window\" es el tamaño de la ventana. windows = 10, usa 10 palabras a la izquierda y 10 palabras a la derecha\n",
    "# \"n_dim\" es la dimension (i.e. el largo) de los vectores de word2vec\n",
    "# \"workers\" es el numero de cores que usa en paralelo. Para aprobechar eso es necesario tener instalado Cython)\n",
    "# \"sample\": word2vec filtra palabras que aparecen una fraccion mayor que \"sample\"\n",
    "# \"min_count\": Word2vec filtra palabras con menos apariciones que  \"min_count\"\n",
    "# \"sg\": para correr el Skipgram model (sg = 1), para correr el CBOW (sg = 0)\n",
    "# para mas detalle ver: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "w2v_model = Word2Vec(trainset_ngrams, workers=4,size=20, min_count = 10, window = 10, sample = 1e-3,negative=5,sg=1)\n",
    "#w2v_model.save(\"word2vec_20dim\")  # save model\n",
    "#w2v_model = Word2Vec.load(\"word2vec_20dim\")  # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHk-nT34m98B"
   },
   "outputs": [],
   "source": [
    "# the output of a word2vec representation is a numpy array \n",
    "w2v_model.wv[\"awesome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iD9c1VO6atQg"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.n_similarity([\"jim_carrey\"], [\"silly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75KNSo61m98E"
   },
   "outputs": [],
   "source": [
    "print (\"morgan_freeman-god similarity:\",w2v_model.wv.n_similarity([\"morgan_freeman\"], [\"god\"]))\n",
    "print (\"jim_carrey-god similarity:\",w2v_model.wv.n_similarity([\"jim_carrey\"], [\"god\"]) )\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "print (\"morgan_freeman-silly similarity:\",w2v_model.wv.n_similarity([\"morgan_freeman\"], [\"silly\"]))\n",
    "print (\"jim_carrey-silly similarity:\",w2v_model.wv.n_similarity([\"jim_carrey\"], [\"silly\"]) )\n",
    "print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csrUhYDJm98H"
   },
   "outputs": [],
   "source": [
    "w2v_model.most_similar(positive=[\"jim_carrey\"], negative=[], topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37DjF48Zm98N"
   },
   "outputs": [],
   "source": [
    "target_word=\"funny\"\n",
    "actores = [\"adam_sandler\",\"jim_carrey\",\"ben_stiller\",\"eddie_murphy\",\"mike_myers\",\"chris_rock\",\"stallone\",\"willis\",\"jackie_chan\",'jet_li',\"van_damme\",\"harrison_ford\",\"schwarzenegger\",\"chuck_norris\",'dicaprio','viggo_mortensen']\n",
    "fun_score = []\n",
    "for actor in actores:\n",
    "    fun_score.append(w2v_model.wv.n_similarity([target_word], [actor]))\n",
    "    \n",
    "pd.DataFrame(fun_score,index = actores,columns=[target_word]).sort_values(by=target_word).plot(kind=\"bar\",figsize=(15,5), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTKkJhmvm98Q"
   },
   "outputs": [],
   "source": [
    "p_movies = [\"horror\",\"comedy\",\"drama\",\"science_fiction\",\"western\",\"documentary\"]\n",
    "p_actors_action = [\"stallone\",\"willis\",\"jackie_chan\",'jet_li',\"van_damme\",\"harrison_ford\",\"schwarzenegger\",\"chuck_norris\"]\n",
    "p_actors_comedy = [\"adam_sandler\",\"jim_carrey\",\"ben_stiller\",\"eddie_murphy\",\"mike_myers\",\"chris_rock\"]\n",
    "p_superheroes = [\"batman\",\"superman\",\"spiderman\",\"robocop\",\"hulk\",\"wolverine\"]\n",
    "p_colors = [\"blue\", \"green\", \"yellow\", \"red\", \"orange\"] # p_robos_en\n",
    "palabras = p_movies + p_actors_action + p_actors_comedy + p_superheroes + p_colors\n",
    "colores = [\"black\"]*len(p_movies)+[\"blue\"]*len(p_actors_action)+[\"green\"]*len(p_actors_comedy)+[\"red\"]*len(p_superheroes) +[\"purple\"]*len(p_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43lS8JmWm98W"
   },
   "outputs": [],
   "source": [
    "# Armo una matriz de distancias\n",
    "distancias=np.zeros((len(palabras),len(palabras))) #matriz cuadrada\n",
    "for i,ti in enumerate(palabras):\n",
    "    for j,tj in enumerate(palabras):\n",
    "        distancias[i,j] = abs(1-w2v_model.wv.similarity(ti,tj))\n",
    "print (distancias.shape)\n",
    "distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbQSOA0Ym98Y"
   },
   "outputs": [],
   "source": [
    "# Reduccion de la dimensionalidad y visualizacion \n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE \n",
    "def visualize_embeddings(distancias,palabras,colores,perplexity):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    # Reduccion de la dimensionalidad y visualizacion \n",
    "    mds = MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=123,\n",
    "                       dissimilarity=\"precomputed\", n_jobs=4)\n",
    "    Y = mds.fit(distancias).embedding_\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(Y[:, 0], Y[:, 1],color=\"black\",s=3)\n",
    "    for label, x, y, color in zip(palabras, Y[:, 0], Y[:, 1],colores):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0),color=color, textcoords='offset points',size=13)\n",
    "    plt.title(\"MDS\")\n",
    "    # Reduccion de la dimensionalidad y visualizacion \n",
    "    tsne = TSNE(n_components=2,metric=\"precomputed\",learning_rate=1000, random_state=123,perplexity=perplexity)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    plt.subplot(1,2,2)\n",
    "    Y = tsne.fit_transform(distancias)\n",
    "    plt.scatter(Y[:, 0], Y[:, 1],color=\"black\",s=3)\n",
    "    for label, x, y, color in zip(palabras, Y[:, 0], Y[:, 1],colores):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0),color=color, textcoords='offset points',size=13)\n",
    "    plt.title(\"TSNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eq_Cgblxm98b"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(distancias,palabras,colores,perplexity=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IreAR5F9m98e"
   },
   "source": [
    "# Latent Semantic Analisis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27061n06m98f"
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary wich maps tokens with Ids\n",
    "dictionary = corpora.Dictionary(trainset)\n",
    "# filter words with low frequency\n",
    "dictionary.filter_extremes(no_below=10, no_above=1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ETmw0Sam98l"
   },
   "outputs": [],
   "source": [
    "list(dictionary.iteritems())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDmUxb2lm98t"
   },
   "outputs": [],
   "source": [
    "# me quedo con todos los tokens menos los \"bad_ids\" o sino con todos los \"good_ids\n",
    "# stopwords_id =np.array(dictionary.doc2bow(stopwords.words('english')))[:,0]\n",
    "# dictionary.filter_tokens(bad_ids=stopwords_id, good_ids=None) \n",
    "# dictionary.save(\"diarios_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ0rDPZtm98w"
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(line) for line in trainset]\n",
    "tfidf = models.TfidfModel(corpus)  # tf-idf  transformation\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtuSjBGZm98y"
   },
   "outputs": [],
   "source": [
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ef5HfJ5bm981"
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "lsa_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics = n_topics)  # initialize an LSI transformation\n",
    "#lsa_tfidf.save(\"LSA_movies.lsi\") # Save LSA\n",
    "#lsa_tfidf = models.LsiModel.load(\"LSA_movies.lsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmrTrZEVm99G"
   },
   "outputs": [],
   "source": [
    "# vectorial representation of a token\n",
    "vect_zombie = lsa_tfidf[dictionary.doc2bow([\"zombie\"])] \n",
    "vect_zombie[:10] # shows only the first 10 elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MERayaYm99J"
   },
   "outputs": [],
   "source": [
    "# show the components of topic 2\n",
    "lsa_tfidf.show_topic(60, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnhXVtyJm99L"
   },
   "outputs": [],
   "source": [
    "# Armo una matriz de distancias\n",
    "distancias_lsa=np.zeros((len(palabras),len(palabras))) #matriz cuadrada\n",
    "for i,ti in enumerate(palabras):\n",
    "    for j,tj in enumerate(palabras):\n",
    "        distancias_lsa[i,j] = abs(1-cossim(lsa_tfidf[dictionary.doc2bow([ti])] ,lsa_tfidf[dictionary.doc2bow([tj])]))\n",
    "print( distancias_lsa.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUkkdEMum99O"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(distancias_lsa,palabras,colores,perplexity=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_Y8lWox-P9Y"
   },
   "source": [
    "# Pretrained Word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaryRDelm99U"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H51mHo4R-UQW"
   },
   "outputs": [],
   "source": [
    "w2v_model2 = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uPO7sxPm99Z"
   },
   "outputs": [],
   "source": [
    "p_robos_en = [\"robberies\", \"weapons\", \"murder\", \"thieves\", \"robbery\", \"assault\"]\n",
    "p_ciencias_en = [\"biology\", \"chemistry\", \"mathematics\", \"philosophy\", \"psychology\", \"science\", \"engineering\"]\n",
    "p_tiempo_en = [\"rainy\", \"sunny\", \"heat\", \"cloudy\", \"snow\", \"storm\"]\n",
    "p_paises_en = [\"Switzerland\", \"Sweden\", \"France\", \"Netherlands\", \"Peru\", \"Bolivia\", \"Paraguay\", \"Uruguay\",\"Argentine\" ,\"Brazil\", \"Colombia\"]\n",
    "p_comida_en = [\"bread\", \"noodles\", \"cookies\", \"cheese\", \"pizza\", \"beer\", \"wine\"]\n",
    "p_tecno_en = [\"technology\", \"computer\", \"internet\", \"web\", \"hackers\", \"monitor\", \"mouse\"]\n",
    "p_hogar_en = [\"kitchen\", \"bathroom\", \"dining_room\", \"armchairs\", \"wardrobe\", \"chairs\", \"tables\", \"tableware\"]\n",
    "grupos = [p_robos_en,p_ciencias_en,p_tiempo_en,p_paises_en,p_comida_en,p_tecno_en,p_hogar_en]\n",
    "palabras = p_robos_en + p_ciencias_en + p_tiempo_en + p_paises_en + p_comida_en + p_tecno_en +p_hogar_en\n",
    "colores = [\"black\"]*len(p_robos_en)+[\"blue\"]*len(p_ciencias_en)+[\"green\"]*len(p_tiempo_en)+[\"red\"]*len(p_paises_en) +[\"purple\"]*len(p_comida_en)+[\"orange\"]*len(p_tecno_en)+[\"cyan\"]*len(p_hogar_en) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bR77sDOTm99b"
   },
   "outputs": [],
   "source": [
    "# Armo una matriz de distancias\n",
    "distancias_pre=np.zeros((len(palabras),len(palabras))) #matriz cuadrada\n",
    "for i,ti in enumerate(palabras):\n",
    "    for j,tj in enumerate(palabras):\n",
    "        distancias_pre[i,j] = abs(1-w2v_model2.wv.similarity(ti,tj))\n",
    "print (distancias_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgn6tB_Nm99e"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(distancias_pre,palabras,colores,perplexity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxxEKa71m99j"
   },
   "outputs": [],
   "source": [
    "print (\"woman-kitchen similarity:\",w2v_model2.wv.n_similarity([\"woman\"], [\"kitchen\"]))\n",
    "print (\"man-kitchen similarity:\",w2v_model2.wv.n_similarity([\"man\"], [\"kitchen\"]) )\n",
    "print (\"\\n\")\n",
    "print (\"woman-wife similarity:\",w2v_model2.wv.n_similarity([\"woman\"], [\"wife\"]) )\n",
    "print (\"man-husband similarity:\",w2v_model2.wv.n_similarity([\"man\"], [\"husband\"]) )\n",
    "print(\"\\n\")\n",
    "print (\"woman-children similarity:\",w2v_model2.wv.n_similarity([\"woman\"], [\"children\"]) )\n",
    "print (\"man-children similarity:\",w2v_model2.wv.n_similarity([\"man\"], [\"children\"]) )\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ou4zLnqvm99m"
   },
   "source": [
    "# Ganando un poco de intuición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNoDgC2wm99m"
   },
   "outputs": [],
   "source": [
    "print (\"bank-money similarity:\",w2v_model2.wv.n_similarity([\"bank\"], [\"money\"]) )\n",
    "print (\"bank-park similarity:\",w2v_model2.wv.n_similarity([\"bank\"], [\"park\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMvB3LOBm99o"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"Argentina\"], [\"park\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N71REK8Ym99s"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"Argentina\"], [\"money\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPYAJFVRm99v"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"run\"], [\"monkey\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVhw5VePm99y"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"wizard\"], [\"happy\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzHaN2hjm990"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"money\"], [\"wealthy\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WptmrO4xm994"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"money\"], [\"bankrupt\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JBbgx4um996"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"good\"], [\"happy\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4bJDt8wm999"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"good\"], [\"bad\"]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOTp2moVm9-A"
   },
   "outputs": [],
   "source": [
    "print(w2v_model2.n_similarity([\"black\"], [\"white\"]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbKQwN0h1be5"
   },
   "source": [
    "# volviendo a los reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJ2TyOIRm9-M"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.n_similarity([\"black\"],[\"white\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdGeoih_m9-Q"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.n_similarity([\"hand\"],[\"orange\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6.word-embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
